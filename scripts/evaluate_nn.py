"""
Dr.Case ‚Äî –û—Ü—ñ–Ω–∫–∞ –º–µ—Ç—Ä–∏–∫ Neural Network

–ü—Ä–∞–≤–∏–ª—å–Ω—ñ –º–µ—Ç—Ä–∏–∫–∏ –∑–≥—ñ–¥–Ω–æ CONFIG_PARAMETERS.md:
- Recall@1, Recall@5, Recall@10
- mAP (mean Average Precision)
- Hamming Loss (–¥–ª—è multilabel)

–ó–∞–ø—É—Å–∫:
    python scripts/evaluate_nn.py
"""

import sys
from pathlib import Path

project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

import numpy as np
import json

import torch
from torch.utils.data import DataLoader

from dr_case.data_generation.two_branch_generator import (
    TwoBranchDataGenerator, 
    TwoBranchSamplerConfig
)
from dr_case.neural_network.two_branch_model import TwoBranchNN, TwoBranchDataset


def recall_at_k(predictions: np.ndarray, targets: np.ndarray, k: int) -> float:
    """
    Recall@k –¥–ª—è single-label classification.
    
    Recall@k = (–∫—ñ–ª—å–∫—ñ—Å—Ç—å samples –¥–µ –ø—Ä–∞–≤–∏–ª—å–Ω–∏–π –∫–ª–∞—Å –≤ —Ç–æ–ø-k) / (–≤—Å—å–æ–≥–æ samples)
    
    –î–ª—è single-label —Ü–µ –µ–∫–≤—ñ–≤–∞–ª–µ–Ω—Ç–Ω–æ "hit rate" –∞–±–æ "top-k accuracy".
    """
    n_samples = len(targets)
    hits = 0
    
    for i in range(n_samples):
        top_k_indices = np.argsort(predictions[i])[-k:][::-1]
        if targets[i] in top_k_indices:
            hits += 1
    
    return hits / n_samples


def average_precision(predictions: np.ndarray, target: int) -> float:
    """
    Average Precision –¥–ª—è –æ–¥–Ω–æ–≥–æ sample (single-label).
    
    AP = 1 / rank(correct_class)
    
    –ß–∏–º –≤–∏—â–µ –ø—Ä–∞–≤–∏–ª—å–Ω–∏–π –∫–ª–∞—Å —É —Ä–∞–Ω–∂—É–≤–∞–Ω–Ω—ñ, —Ç–∏–º –≤–∏—â–µ AP.
    """
    sorted_indices = np.argsort(predictions)[::-1]
    rank = np.where(sorted_indices == target)[0][0] + 1  # 1-based rank
    return 1.0 / rank


def mean_average_precision(predictions: np.ndarray, targets: np.ndarray) -> float:
    """
    Mean Average Precision (mAP).
    
    mAP = mean(AP –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ sample)
    """
    n_samples = len(targets)
    ap_sum = 0.0
    
    for i in range(n_samples):
        ap_sum += average_precision(predictions[i], targets[i])
    
    return ap_sum / n_samples


def mean_reciprocal_rank(predictions: np.ndarray, targets: np.ndarray) -> float:
    """
    Mean Reciprocal Rank (MRR).
    
    MRR = mean(1 / rank(correct_class))
    
    –î–ª—è single-label MRR = mAP.
    """
    return mean_average_precision(predictions, targets)


def rank_distribution(predictions: np.ndarray, targets: np.ndarray) -> dict:
    """
    –†–æ–∑–ø–æ–¥—ñ–ª —Ä–∞–Ω–≥—ñ–≤ –ø—Ä–∞–≤–∏–ª—å–Ω–∏—Ö –∫–ª–∞—Å—ñ–≤.
    """
    ranks = []
    
    for i in range(len(targets)):
        sorted_indices = np.argsort(predictions[i])[::-1]
        rank = np.where(sorted_indices == targets[i])[0][0] + 1
        ranks.append(rank)
    
    ranks = np.array(ranks)
    
    return {
        "min": int(ranks.min()),
        "max": int(ranks.max()),
        "mean": float(ranks.mean()),
        "median": float(np.median(ranks)),
        "std": float(ranks.std()),
        "rank_1": int((ranks == 1).sum()),
        "rank_1_5": int((ranks <= 5).sum()),
        "rank_1_10": int((ranks <= 10).sum()),
        "rank_1_20": int((ranks <= 20).sum()),
    }


def main():
    print("=" * 70)
    print("Dr.Case ‚Äî –û–¶–Ü–ù–ö–ê –ú–ï–¢–†–ò–ö NEURAL NETWORK")
    print("=" * 70)
    
    # –®–ª—è—Ö–∏
    database_path = project_root / "data" / "unified_disease_symptom_merged.json"
    som_path = project_root / "models" / "som_merged.pkl"
    model_path = project_root / "models" / "nn_two_branch.pt"
    output_path = project_root / "models" / "nn_evaluation_metrics.json"
    
    # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Ñ–∞–π–ª—ñ–≤
    for path, name in [(database_path, "Database"), (som_path, "SOM"), (model_path, "NN Model")]:
        if not path.exists():
            print(f"‚ùå {name} –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ: {path}")
            return
    
    print(f"\nüìÅ Model: {model_path}")
    
    # ========== –ó–ê–í–ê–ù–¢–ê–ñ–ï–ù–ù–Ø –ú–û–î–ï–õ–Ü ==========
    
    print("\nüîÑ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ...")
    
    checkpoint = torch.load(model_path, map_location='cpu')
    
    model = TwoBranchNN(**checkpoint['model_config'])
    model.load_state_dict(checkpoint['model_state'])
    model.eval()
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = model.to(device)
    
    print(f"   Device: {device}")
    print(f"   Parameters: {model.count_parameters():,}")
    
    # ========== –ì–ï–ù–ï–†–ê–¶–Ü–Ø –¢–ï–°–¢–û–í–ò–• –î–ê–ù–ò–• ==========
    
    print("\nüîÑ –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è —Ç–µ—Å—Ç–æ–≤–∏—Ö –¥–∞–Ω–∏—Ö...")
    
    generator_config = TwoBranchSamplerConfig(
        samples_per_disease=50,  # –ú–µ–Ω—à–µ –¥–ª—è —à–≤–∏–¥–∫–æ—ó –æ—Ü—ñ–Ω–∫–∏
        min_symptoms=2,
        noise_probability=0.02,
        dropout_probability=0.15,
        som_k=10,
        random_seed=123  # –Ü–Ω—à–∏–π seed –¥–ª—è —Ç–µ—Å—Ç—É
    )
    
    generator = TwoBranchDataGenerator.from_files(
        str(database_path),
        str(som_path),
        generator_config
    )
    
    # –ì–µ–Ω–µ—Ä—É—î–º–æ —Ç–µ—Å—Ç–æ–≤—É –≤–∏–±—ñ—Ä–∫—É
    X_sym, X_som, y, _ = generator.generate(
        samples_per_disease=50,
        verbose=True
    )
    
    print(f"   Test samples: {len(y)}")
    
    # ========== –û–¢–†–ò–ú–ê–ù–ù–Ø PREDICTIONS ==========
    
    print("\nüîÑ –û—Ç—Ä–∏–º–∞–Ω–Ω—è predictions...")
    
    dataset = TwoBranchDataset(
        symptom_vectors=X_sym,
        som_contexts=X_som,
        disease_indices=y,
        augment=False
    )
    
    loader = DataLoader(dataset, batch_size=128, shuffle=False)
    
    all_predictions = []
    all_targets = []
    
    with torch.no_grad():
        for symptoms, som_context, targets in loader:
            symptoms = symptoms.to(device)
            som_context = som_context.to(device)
            
            outputs = model(symptoms, som_context)
            
            # Softmax –¥–ª—è –π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç–µ–π
            probs = torch.softmax(outputs, dim=1)
            
            all_predictions.append(probs.cpu().numpy())
            all_targets.append(targets.numpy())
    
    predictions = np.vstack(all_predictions)
    targets = np.concatenate(all_targets)
    
    print(f"   Predictions shape: {predictions.shape}")
    
    # ========== –û–ë–ß–ò–°–õ–ï–ù–ù–Ø –ú–ï–¢–†–ò–ö ==========
    
    print("\n" + "=" * 70)
    print("üìä –ú–ï–¢–†–ò–ö–ò")
    print("=" * 70)
    
    # Recall@k
    recall_1 = recall_at_k(predictions, targets, k=1)
    recall_5 = recall_at_k(predictions, targets, k=5)
    recall_10 = recall_at_k(predictions, targets, k=10)
    recall_20 = recall_at_k(predictions, targets, k=20)
    
    print(f"\nüìà Recall@k:")
    print(f"   Recall@1:  {recall_1:.2%}  (—Ü—ñ–ª—å –ø—Ä–æ—Ç–æ—Ç–∏–ø—É: 50%, production: 70%)")
    print(f"   Recall@5:  {recall_5:.2%}  (—Ü—ñ–ª—å –ø—Ä–æ—Ç–æ—Ç–∏–ø—É: 85%, production: 95%)")
    print(f"   Recall@10: {recall_10:.2%}  (—Ü—ñ–ª—å –ø—Ä–æ—Ç–æ—Ç–∏–ø—É: 92%, production: 99%)")
    print(f"   Recall@20: {recall_20:.2%}")
    
    # mAP
    map_score = mean_average_precision(predictions, targets)
    mrr_score = mean_reciprocal_rank(predictions, targets)
    
    print(f"\nüìà Ranking Quality:")
    print(f"   mAP (mean Average Precision): {map_score:.4f}  (—Ü—ñ–ª—å –ø—Ä–æ—Ç–æ—Ç–∏–ø—É: 0.60, production: 0.80)")
    print(f"   MRR (Mean Reciprocal Rank):   {mrr_score:.4f}")
    
    # Rank distribution
    rank_dist = rank_distribution(predictions, targets)
    
    print(f"\nüìà Rank Distribution:")
    print(f"   Mean rank: {rank_dist['mean']:.1f}")
    print(f"   Median rank: {rank_dist['median']:.1f}")
    print(f"   Min/Max rank: {rank_dist['min']}/{rank_dist['max']}")
    print(f"   Rank = 1: {rank_dist['rank_1']} ({rank_dist['rank_1']/len(targets):.1%})")
    print(f"   Rank ‚â§ 5: {rank_dist['rank_1_5']} ({rank_dist['rank_1_5']/len(targets):.1%})")
    print(f"   Rank ‚â§ 10: {rank_dist['rank_1_10']} ({rank_dist['rank_1_10']/len(targets):.1%})")
    
    # ========== –ü–û–†–Ü–í–ù–Ø–ù–ù–Ø –ó –¶–Ü–õ–Ø–ú–ò ==========
    
    print("\n" + "=" * 70)
    print("üéØ –ü–û–†–Ü–í–ù–Ø–ù–ù–Ø –ó –¶–Ü–õ–Ø–ú–ò")
    print("=" * 70)
    
    targets_prototype = {
        "Recall@1": 0.50,
        "Recall@5": 0.85,
        "Recall@10": 0.92,
        "mAP": 0.60,
    }
    
    targets_production = {
        "Recall@1": 0.70,
        "Recall@5": 0.95,
        "Recall@10": 0.99,
        "mAP": 0.80,
    }
    
    actual = {
        "Recall@1": recall_1,
        "Recall@5": recall_5,
        "Recall@10": recall_10,
        "mAP": map_score,
    }
    
    print(f"\n{'Metric':<12} {'Actual':>10} {'Prototype':>12} {'Production':>12} {'Status'}")
    print("-" * 60)
    
    for metric in ["Recall@1", "Recall@5", "Recall@10", "mAP"]:
        val = actual[metric]
        proto = targets_prototype[metric]
        prod = targets_production[metric]
        
        if val >= prod:
            status = "‚úÖ Production"
        elif val >= proto:
            status = "‚úÖ Prototype"
        else:
            status = "‚ùå Below"
        
        print(f"{metric:<12} {val:>10.2%} {proto:>12.0%} {prod:>12.0%} {status}")
    
    # ========== –ó–ë–ï–†–ï–ñ–ï–ù–ù–Ø ==========
    
    metrics_data = {
        "model": str(model_path),
        "test_samples": len(targets),
        "recall": {
            "recall_1": recall_1,
            "recall_5": recall_5,
            "recall_10": recall_10,
            "recall_20": recall_20,
        },
        "ranking": {
            "mAP": map_score,
            "MRR": mrr_score,
        },
        "rank_distribution": rank_dist,
        "targets": {
            "prototype": targets_prototype,
            "production": targets_production,
        }
    }
    
    with open(output_path, 'w') as f:
        json.dump(metrics_data, f, indent=2)
    
    print(f"\nüíæ Metrics saved: {output_path}")
    
    print("\n" + "=" * 70)
    print("‚úÖ –û–¶–Ü–ù–ö–ê –ó–ê–í–ï–†–®–ï–ù–ê!")
    print("=" * 70)


if __name__ == "__main__":
    main()
