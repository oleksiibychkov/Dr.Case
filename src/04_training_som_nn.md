[← Повернутися на головну](../)

---

## 33. Навчання SOM на профілях діагнозів

### 33.1. Дані для SOM

Навчальна вибірка для SOM — це множина векторів:
$$\{x_d : d \in \mathcal{D}\}$$

де $x_d \in \{0,1\}^n$ — мультихот-вектор симптомів для діагнозу $d$:
$$(x_d)_i = \begin{cases} 1 & \text{якщо } s_i \in \text{symptoms}(d) \\ 0 & \text{інакше} \end{cases}$$

SOM вчиться **на діагнозах**, представлених як "симптомні профілі".

### 33.2. Алгоритм навчання SOM

1. Вибрати розмір карти $H \times W$
2. Ініціалізувати ваги кожного юніта: $w_u \in \mathbb{R}^n$
3. Для кожної епохи і для кожного $x_d$:
   - знайти BMU: $u^* = \arg\min_u \|x_d - w_u\|$
   - оновити BMU і сусідів:
     $$w_u \leftarrow w_u + \eta(t) \cdot h(u, u^*, \sigma(t)) \cdot (x_d - w_u)$$

де $\eta(t)$ — learning rate, $h$ — функція сусідства (гаусова), $\sigma(t)$ — радіус.

---

## 34. Гіперпараметри SOM та їх обґрунтування

### 34.1. Розмір карти $(H \times W)$

**Емпіричне правило Vesanto:**
$$H \times W \approx 5\sqrt{N}$$

де $N$ — кількість діагнозів у базі.

| Кількість діагнозів $N$ | Рекомендований розмір | Кількість юнітів |
|-------------------------|----------------------|------------------|
| 100 | 7×7 або 8×8 | 49–64 |
| 400 | 10×10 | 100 |
| 1000 | 15×15 або 16×16 | 225–256 |
| 2500 | 25×25 | 625 |

**Обґрунтування:** занадто мала карта призводить до того, що різні діагнози "злипаються" в одному юніті; занадто велика — до порожніх юнітів і втрати топологічної структури.

### 34.2. Learning rate $\eta(t)$

**Експоненційний розпад:**
$$\eta(t) = \eta_0 \cdot \exp\left(-\frac{t}{T_\eta}\right)$$

**Рекомендовані значення:**
- $\eta_0 = 0.5$ (початковий learning rate)
- $\eta_{final} \geq 0.01$ (фінальний, не менше)
- $T_\eta = T_{max} / 3$ (константа розпаду)

### 34.3. Радіус сусідства $\sigma(t)$

**Експоненційний розпад:**
$$\sigma(t) = \sigma_0 \cdot \exp\left(-\frac{t}{T_\sigma}\right)$$

**Рекомендовані значення:**
- $\sigma_0 = \max(H, W) / 2$ (початковий радіус)
- $\sigma_{final} \geq 0.5$ (фінальний)
- $T_\sigma = T_{max} / 4$

### 34.4. Функція сусідства $h(u, u^*, \sigma)$

**Гаусова функція (стандарт):**
$$h(u, u^*, \sigma) = \exp\left(-\frac{\|r_u - r_{u^*}\|^2}{2\sigma^2}\right)$$

де $r_u$ — координати юніта $u$ на карті.

**Bubble-функція (спрощена):**
$$h(u, u^*, \sigma) = \begin{cases} 1 & \text{якщо } \|r_u - r_{u^*}\| \leq \sigma \\ 0 & \text{інакше} \end{cases}$$

**Рекомендація:** гаусова функція дає плавніші карти і краще зберігає топологію.

### 34.5. Кількість епох $T_{max}$

**Емпіричне правило:**
$$T_{max} \geq 500 \cdot (H \times W) / N$$

- Мінімум: 100 епох
- Рекомендовано: 500–1000 епох
- Для великих карт: до 2000 епох

**Критерій ранньої зупинки:** якщо квантизаційна помилка не зменшується протягом 50 епох.

### 34.6. Ініціалізація ваг

**Випадкова ініціалізація:**
$$w_u \sim \mathcal{U}[\min(X), \max(X)]$$

**PCA-ініціалізація (рекомендовано):**
1. Обчислити перші 2 головні компоненти даних
2. Розмістити ваги юнітів вздовж цих компонент

PCA-ініціалізація значно пришвидшує збіжність і дає стабільніші результати.

---

## 35. Метрики якості навченої SOM

### 35.1. Квантизаційна помилка (Quantization Error)

$$QE = \frac{1}{N} \sum_{d=1}^{N} \|x_d - w_{\text{BMU}(x_d)}\|$$

**Інтерпретація:** середня відстань від кожного діагнозу до його BMU. Чим менше — тим краще.

**Очікувані значення:** для L2-нормованих векторів $QE \in [0.1, 0.5]$ — прийнятно.

### 35.2. Топографічна помилка (Topographic Error)

$$TE = \frac{1}{N} \sum_{d=1}^{N} \mathbf{1}[\text{BMU}_1(x_d) \text{ і } \text{BMU}_2(x_d) \text{ не сусіди}]$$

**Порогові значення:**
- $TE < 0.1$ — відмінно
- $TE \in [0.1, 0.2]$ — прийнятно
- $TE > 0.3$ — топологія порушена, потрібно перенавчати

### 35.3. Середня кількість діагнозів на юніт

$$\bar{D} = \frac{N}{|\{u : \Gamma(u) \neq \emptyset\}|}$$

**Рекомендовані значення:**
- $\bar{D} \in [2, 10]$ — оптимально
- $\bar{D} > 20$ — карта занадто мала
- $\bar{D} < 1.5$ — карта занадто велика

### 35.4. Коефіцієнт заповнення карти

$$\text{Fill} = \frac{|\{u : \Gamma(u) \neq \emptyset\}|}{H \times W}$$

**Рекомендовані значення:**
- $\text{Fill} \in [0.6, 0.95]$ — оптимально

---

## 36. Алгоритм навчання SOM (повний псевдокод)

```
Вхід:
  - X = {x_d : d ∈ D} — множина векторів діагнозів
  - H, W — розміри карти
  - T_max — кількість епох
  - η₀, σ₀ — початкові learning rate і радіус

Ініціалізація:
  1. Для кожного юніта u ∈ {1..H} × {1..W}:
     w_u ← PCA_init(X, u) або Random_init(X)

Навчання:
  2. Для t = 1 до T_max:
     2.1. η(t) ← η₀ · exp(-t / T_η)
     2.2. σ(t) ← σ₀ · exp(-t / T_σ)
     2.3. Перемішати X
     2.4. Для кожного x_d ∈ X:
          a) Знайти BMU: u* = argmin_u ‖x_d - w_u‖
          b) Для кожного юніта u:
             h ← exp(-‖r_u - r_{u*}‖² / (2σ(t)²))
             w_u ← w_u + η(t) · h · (x_d - w_u)
     2.5. Обчислити QE(t)
     2.6. Якщо QE не зменшується 50 епох — BREAK

Побудова мапи:
  3. Для кожного d ∈ D:
     u_d ← argmin_u ‖x_d - w_u‖
     Γ(u_d) ← Γ(u_d) ∪ {d}

Валідація:
  4. Обчислити QE, TE, Fill
  5. Перевірити пороги якості

Вихід:
  - {w_u} — ваги юнітів
  - Γ — мапа "юніт → діагнози"
  - Метрики якості
```

---

## 37. Генерація псевдопацієнтських прикладів

### 37.1. Необхідність генерації псевдокейсів

База знань "діагноз → симптоми" дає ідеальні профілі. Але реальний пацієнт:
- називає не всі симптоми (неповнота);
- може переплутати формулювання (шум);
- може мати кілька проблем одночасно (коморбідність).

Тому ми генеруємо **псевдопацієнтські приклади**, які навмисно "псують" ідеальний профіль.

### 37.2. Dropout симптомів (неповний опис)

Для кожного активного симптому $s_i$ у $x_d$ з ймовірністю $p_{\text{drop}}$ вимикаємо його:
$$\tilde{x} = \mathrm{DropSymptoms}(x_d, p_{\text{drop}})$$

**Математична модель:**
$$\tilde{x}_i = x_{d,i} \cdot \text{Bernoulli}(1 - p_{\text{drop}})$$

**Рекомендовані параметри:**

| Фаза діагностики | $p_{\text{drop}}$ | Обґрунтування |
|------------------|-------------------|---------------|
| Початкова скарга | 0.5–0.7 | Пацієнт називає 30–50% симптомів |
| Після уточнень | 0.2–0.4 | Лікар виявив більше симптомів |
| Повний огляд | 0.0–0.1 | Майже всі симптоми відомі |

### 37.3. Додавання шуму (зайві симптоми)

**Кількість шумових симптомів:**

*Фіксована кількість:* $q_{\text{add}} \in \{0, 1, 2, 3\}$ з розподілом:
$$P(q_{\text{add}} = k) = \begin{cases} 0.5 & k = 0 \\ 0.25 & k = 1 \\ 0.15 & k = 2 \\ 0.10 & k = 3 \end{cases}$$

*Пропорційно до кількості симптомів:*
$$q_{\text{add}} \sim \text{Poisson}(\lambda \cdot |x_d|_1)$$
де $\lambda \in [0.1, 0.3]$.

**Вибір шумових симптомів:**

*Метод 1: Рівномірний*
$$s_{\text{noise}} \sim \text{Uniform}(\mathcal{S} \setminus S(d))$$

*Метод 2: Частотно-зважений*
$$P(s_{\text{noise}} = s) \propto \text{freq}(s)$$

*Метод 3: SOM-локальний (найреалістичніший)*
$$s_{\text{noise}} \sim \text{Uniform}\left(\bigcup_{u \in \text{neighbors}(\text{BMU}(x_d))} S(\Gamma(u))\right)$$

### 37.4. Коморбідність (мультидіагноз)

**Ймовірність мультидіагнозу:**

| Кількість діагнозів | Ймовірність |
|---------------------|-------------|
| 1 (single-label) | 0.70 |
| 2 (коморбідність) | 0.25 |
| 3+ (складний випадок) | 0.05 |

**Генерація:**
- вибираємо $d_1$ та $d_2$,
- об'єднуємо симптоми: $\tilde{x} = x_{d_1} \lor x_{d_2}$,
- формуємо мітку: $y_{d_1} = 1,\ y_{d_2} = 1$.

### 37.5. Симуляція ітеративності

**Кількість кроків уточнення:**
$$T \sim \text{Uniform}(\{1, 2, 3, 4, 5\})$$

**Кількість симптомів на кроці:**
$$\Delta_t \sim \text{Poisson}(2) + 1$$

**Модель послідовності:**
$$x^{(0)} \subset x^{(1)} \subset \ldots \subset x^{(T)} \subseteq x_d$$

### 37.6. Рекомендований розподіл навчальної вибірки

| Тип прикладу | Частка | Параметри |
|--------------|--------|-----------|
| Single-label, висока неповнота | 30% | $p_{\text{drop}} = 0.6$, $q_{\text{add}} = 0$ |
| Single-label, помірна неповнота | 25% | $p_{\text{drop}} = 0.3$, $q_{\text{add}} = 1$ |
| Single-label, з шумом | 15% | $p_{\text{drop}} = 0.4$, $q_{\text{add}} = 2$ |
| Multi-label (2 діагнози) | 20% | $p_{\text{drop}} = 0.3$, $q_{\text{add}} = 1$ |
| Multi-label (3 діагнози) | 5% | $p_{\text{drop}} = 0.2$, $q_{\text{add}} = 1$ |
| Ітеративні послідовності | 5% | $T \in \{2, 3, 4\}$ |

### 37.7. Кількість псевдокейсів

**Емпіричне правило:**
$$N_{\text{pseudo}} = k \cdot |\mathcal{D}|$$

де $k \in [10, 50]$ — кількість псевдокейсів на один діагноз.

Для бази з 400 діагнозів:
- Мінімум: 4,000 псевдокейсів
- Рекомендовано: 10,000–20,000 псевдокейсів
- Для production: 50,000+ псевдокейсів

---

## 38. Побудова SOM-контексту

### 38.1. Обчислення контексту

Для прикладу $\tilde{x}$ обчислюємо відстань до кожного юніта:
$$d_u = \|\tilde{x} - w_u\|$$

Перетворюємо відстані на membership (softmax):
$$m_u = \frac{\exp(-d_u^2/\lambda)}{\sum_v \exp(-d_v^2/\lambda)}$$

де $\lambda$ керує "гостроту" розподілу.

### 38.2. Top-k стиснення

Практичний компроміс: брати **top-k юнітів**, а решту обнуляти:
- знаходимо $k$ юнітів з найменшими $d_u$,
- обчислюємо softmax лише на них,
- для інших $m_u = 0$.

---

## 39. Архітектура Multilabel NN

### 39.1. Формування входу

Вхідний вектор:
$$z = [\tilde{x}; m]$$

Розмірність:
$$\dim(z) = n + k$$

де $n$ — розмір словника симптомів, $k$ — кількість top-k юнітів.

### 39.2. Базова архітектура (MLP)

$$h = \phi(W_1 z + b_1), \quad \ell = W_2 h + b_2, \quad \hat{y} = \sigma(\ell)$$

де $\phi$ — ReLU/GeLU, $\hat{y}_i$ — узгодженість діагнозу $d_i$.

### 39.3. Двогілкова архітектура (рекомендована)

```
Branch A (симптоми):          Branch B (SOM-контекст):
  x̃ ∈ ℝ^n                        m ∈ ℝ^k
    ↓                               ↓
  Dense(256, ReLU)                Dense(64, ReLU)
    ↓                               ↓
  Dropout(0.3)                    Dropout(0.2)
    ↓                               ↓
  Dense(128, ReLU)                Dense(32, ReLU)
    ↓                               ↓
  h_x ∈ ℝ^128                     h_m ∈ ℝ^32
          ↘                     ↙
              Concatenate
                  ↓
              h = [h_x; h_m] ∈ ℝ^160
                  ↓
              Dense(128, ReLU) + Dropout(0.3)
                  ↓
              Dense(L, Sigmoid)
                  ↓
              Output: ŷ ∈ [0,1]^L
```

**Обґрунтування розмірностей:**
- Гілка A (симптоми): 256 → 128, бо симптомів багато
- Гілка B (SOM): 64 → 32, бо SOM — вже стиснене представлення
- Співвідношення: $\dim(h_x) : \dim(h_m) \approx 4:1$

### 39.4. Архітектура для малих датасетів ($N < 5000$)

```
Input: z ∈ ℝ^(n+k)
  ↓
Dense(256, ReLU) + Dropout(0.3)
  ↓
Dense(128, ReLU) + Dropout(0.3)
  ↓
Dense(L, Sigmoid)
  ↓
Output: ŷ ∈ [0,1]^L
```

### 39.5. Архітектура для середніх датасетів ($N \in [5000, 50000]$)

```
Input: z ∈ ℝ^(n+k)
  ↓
Dense(512, ReLU) + BatchNorm + Dropout(0.4)
  ↓
Dense(256, ReLU) + BatchNorm + Dropout(0.3)
  ↓
Dense(128, ReLU) + Dropout(0.2)
  ↓
Dense(L, Sigmoid)
  ↓
Output: ŷ ∈ [0,1]^L
```

---

## 40. Функція втрат (Loss)

### 40.1. Binary Cross-Entropy

$$\mathcal{L}(y, \hat{y}) = -\sum_{i=1}^L \left( y_i \log \hat{y}_i + (1-y_i) \log(1-\hat{y}_i) \right)$$

### 40.2. Зважена BCE (для незбалансованих класів)

$$\mathcal{L} = -\sum_i w_i \left( y_i \log \hat{y}_i + (1-y_i) \log(1-\hat{y}_i) \right)$$

### 40.3. Label Smoothing (опціонально)

Замість жорстких міток $y \in \{0, 1\}$:
$$y_{\text{smooth}} = y \cdot (1 - \epsilon) + \frac{\epsilon}{2}$$

де $\epsilon \in [0.05, 0.1]$.

---

## 41. Гіперпараметри навчання Multilabel NN

### 41.1. Оптимізатор

**Adam (рекомендовано):**
$$\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t$$

Параметри:
- Learning rate: $\eta = 10^{-3}$
- $\beta_1 = 0.9$
- $\beta_2 = 0.999$
- $\epsilon = 10^{-8}$

**AdamW (для великих моделей):**
- Weight decay: $\lambda = 10^{-4}$

### 41.2. Learning Rate Schedule

**Warm-up + Cosine Decay:**

$$\eta(t) = \begin{cases}
\eta_{\max} \cdot \frac{t}{T_{\text{warmup}}} & t < T_{\text{warmup}} \\
\eta_{\min} + \frac{1}{2}(\eta_{\max} - \eta_{\min})(1 + \cos(\frac{t - T_{\text{warmup}}}{T_{\max} - T_{\text{warmup}}} \pi)) & t \geq T_{\text{warmup}}
\end{cases}$$

Параметри:
- $\eta_{\max} = 10^{-3}$
- $\eta_{\min} = 10^{-6}$
- $T_{\text{warmup}} = 5$ епох

### 41.3. Batch Size

| Розмір датасету | Batch size | Обґрунтування |
|-----------------|------------|---------------|
| < 5,000 | 32 | Більше шуму → краща регуляризація |
| 5,000–50,000 | 64–128 | Баланс швидкості та стабільності |
| > 50,000 | 256–512 | Швидше навчання |

### 41.4. Кількість епох

| Розмір датасету | Рекомендовано епох | З early stopping |
|-----------------|-------------------|------------------|
| < 5,000 | 100–200 | patience=10 |
| 5,000–50,000 | 50–100 | patience=7 |
| > 50,000 | 20–50 | patience=5 |

### 41.5. Ініціалізація ваг

**He (для ReLU):**
$$W \sim \mathcal{N}\left(0, \sqrt{\frac{2}{n_{in}}}\right)$$

**Вихідний шар (для незбалансованих класів):**
$$b_i = \log\left(\frac{p_i}{1 - p_i}\right)$$

де $p_i$ — частота діагнозу $i$.

### 41.6. Gradient Clipping

$$\nabla_\theta \leftarrow \frac{\nabla_\theta}{\max(1, \|\nabla_\theta\| / \text{max\_norm})}$$

Рекомендовано: $\text{max\_norm} = 1.0$

### 41.7. Повний набір рекомендованих гіперпараметрів

```python
config = {
    # Архітектура
    "hidden_dims": [512, 256, 128],
    "dropout_rates": [0.4, 0.3, 0.2],
    "activation": "relu",
    "use_batchnorm": True,
    
    # Оптимізація
    "optimizer": "adamw",
    "learning_rate": 1e-3,
    "weight_decay": 1e-4,
    "batch_size": 64,
    "max_epochs": 100,
    
    # Регуляризація
    "label_smoothing": 0.05,
    "gradient_clip": 1.0,
    
    # Early stopping
    "patience": 10,
    "min_delta": 1e-4,
    
    # LR schedule
    "scheduler": "cosine",
    "warmup_epochs": 5,
    "min_lr": 1e-6,
}
```

---

## 42. Алгоритм навчання Multilabel NN (повний псевдокод)

```
Вхід:
  - train_data: список пар (z, y)
  - val_data: список пар (z, y) для валідації
  - config: гіперпараметри

Ініціалізація:
  1. model ← create_model(config)
  2. optimizer ← AdamW(model.params, lr=config.lr, weight_decay=config.wd)
  3. scheduler ← CosineScheduler(optimizer, config)
  4. best_val_loss ← ∞
  5. patience_counter ← 0

Навчання:
  6. Для epoch = 1 до config.max_epochs:
     
     # Training
     6.1. model.train()
     6.2. Перемішати train_data
     6.3. Для кожного batch (Z, Y) з train_data:
          a) Ŷ ← model(Z)
          b) loss ← BCE(Y, Ŷ) + config.label_smoothing * entropy(Ŷ)
          c) loss.backward()
          d) clip_gradients(model, config.gradient_clip)
          e) optimizer.step()
          f) optimizer.zero_grad()
     
     # Validation
     6.4. model.eval()
     6.5. val_loss ← compute_loss(model, val_data)
     6.6. metrics ← compute_metrics(model, val_data)
     
     # Logging
     6.7. log(epoch, train_loss, val_loss, metrics)
     
     # Early stopping
     6.8. Якщо val_loss < best_val_loss - config.min_delta:
          best_val_loss ← val_loss
          save_model(model, "best_model.pt")
          patience_counter ← 0
          Інакше:
          patience_counter ← patience_counter + 1
     
     6.9. Якщо patience_counter ≥ config.patience:
          log("Early stopping at epoch", epoch)
          BREAK
     
     # LR update
     6.10. scheduler.step()

Фіналізація:
  7. model ← load_model("best_model.pt")
  8. final_metrics ← compute_metrics(model, val_data)
  9. Повернути model, final_metrics
```

---

## 43. Валідація Multilabel NN

### 43.1. Розподіл даних

| Вибірка | Частка | Призначення |
|---------|--------|-------------|
| Train | 70-80% | Навчання (backprop) |
| Validation | 10-15% | Підбір гіперпараметрів, early stopping |
| Test | 10-15% | Фінальна оцінка (один раз наприкінці) |

### 43.2. Метрики валідації

#### BCE Loss

$$\mathcal{L}_{\text{BCE}} = -\frac{1}{N \cdot L}\sum_{j=1}^{N}\sum_{i=1}^{L} \left( y_i^{(j)}\log \hat{y}_i^{(j)} + (1-y_i^{(j)})\log(1-\hat{y}_i^{(j)}) \right)$$

#### Recall@k (головна метрика для діагностики)

$$\text{Recall@}k = \frac{1}{N}\sum_{j=1}^{N} \frac{|Y^{(j)} \cap \text{Top}_k(\hat{y}^{(j)})|}{|Y^{(j)}|}$$

**Чому Recall@k важливий:** у диференціальній діагностиці критично, щоб істинний діагноз **був у списку гіпотез**.

#### Precision@k

$$\text{Precision@}k = \frac{1}{N}\sum_{j=1}^{N} \frac{|Y^{(j)} \cap \text{Top}_k(\hat{y}^{(j)})|}{k}$$

#### Mean Average Precision (mAP)

$$\text{mAP} = \frac{1}{L}\sum_{d=1}^{L} \text{AP}_d$$

### 43.3. Рекомендовані пороги якості

| Метрика | Мінімум для прототипу | Ціль для production |
|---------|----------------------|---------------------|
| Recall@1 | ≥ 0.50 | ≥ 0.70 |
| Recall@5 | ≥ 0.85 | ≥ 0.95 |
| Recall@10 | ≥ 0.92 | ≥ 0.99 |
| mAP | ≥ 0.60 | ≥ 0.80 |
| Hamming Loss | ≤ 0.10 | ≤ 0.05 |

**Примітка:** Recall@k важливіший за Precision@k, бо пропустити істинний діагноз небезпечніше, ніж включити зайвий.

### 43.4. Діагностика проблем

| Симптом | Можлива причина | Рішення |
|---------|-----------------|---------|
| Val loss >> Train loss | Перенавчання | Збільшити dropout, зменшити модель |
| Обидва loss великі | Недонавчання | Збільшити модель, більше епох |
| Recall@1 низький, Recall@10 високий | Модель "розмазує" ймовірності | Зменшити λ, temperature scaling |
| Recall низький для рідкісних діагнозів | Дисбаланс класів | Збільшити ваги рідкісних класів |

### 43.5. Early Stopping

**Параметри:**
- `patience`: 5-10 епох
- `min_delta`: 0.001

---

## 44. Повний навчальний пайплайн

1. Прочитати JSON "діагноз → симптоми"
2. Побудувати словник симптомів $\mathcal{S}$
3. Побудувати вектори $x_d$ для всіх діагнозів
4. Навчити SOM на $\{x_d\}$
5. Побудувати мапу $\Gamma(u)$ (нейрон → діагнози)
6. Згенерувати псевдопацієнтські приклади $\tilde{x}$
7. Для кожного $\tilde{x}$ обчислити SOM-контекст $m$
8. Зібрати входи $z = [\tilde{x}; m]$ і мітки $y$
9. Навчити Multilabel NN на цих парах
10. Валідувати Multilabel NN на held-out псевдокейсах
11. (Опційно) Донавчити на реальних пацієнтських кейсах

---

## 45. Приклад звіту валідації

```
========== Validation Report ==========
Dataset: val_pseudocases_v1
Samples: 2,847
Diagnoses: 423

Loss:
  BCE Loss (train): 0.0423
  BCE Loss (val):   0.0512  [+21% — acceptable]

Recall@k:
  Recall@1:  0.67
  Recall@3:  0.89
  Recall@5:  0.94  ✓ [≥ 0.85]
  Recall@10: 0.98  ✓ [≥ 0.92]

Precision@k:
  Precision@1: 0.67
  Precision@5: 0.31

Ranking:
  mAP: 0.72  ✓ [≥ 0.60]

Status: PASS — модель готова до наступного етапу
========================================
```

---

# ВИСНОВКИ

Цей документ містить повну технічну специфікацію експертної системи медичної диференціальної діагностики, побудованої на ієрархічній архітектурі SOM + Multilabel Neural Network.

**Ключові компоненти системи:**

1. **SOM** — структурує простір симптомів, створює карту клінічних сценаріїв
2. **Candidate Selector** — безпечний фільтр гіпотез з формальними гарантіями
3. **Механізм питань** — активне зняття невизначеності через Expected Information Gain
4. **Multilabel NN** — точна ймовірнісна оцінка діагнозів у контексті SOM

**Формальні гарантії системи:**

- Теорема про cumulative mass з повним доведенням
- Нижня межа Candidate Recall через частку "поганих" кейсів
- Ймовірнісні гарантії через концентрацію Хеффдінга
- Гарантії через емпіричний квантиль рангу (DKW-нерівність)

**Рекомендовані параметри:**

- Candidate Selector: $\alpha = 0.9$, $k = 6$, $\tau = 0.01$
- SOM: розмір $\approx 5\sqrt{N}$, $\eta_0 = 0.5$, $\sigma_0 = \max(H,W)/2$
- Multilabel NN: двогілкова архітектура, AdamW, early stopping

Разом ці компоненти утворюють **експертну систему**, а не просто модель — систему, яка імітує клінічне мислення і забезпечує доказовість своїх рішень.
