"""
Dr.Case ‚Äî Full Pipeline Tuner

End-to-end –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è –≤—Å—å–æ–≥–æ pipeline:
1. –û—Ü—ñ–Ω–∫–∞ –ø–æ—Ç–æ—á–Ω–æ—ó —è–∫–æ—Å—Ç—ñ
2. –Ü–¥–µ–Ω—Ç–∏—Ñ—ñ–∫–∞—Ü—ñ—è —Å–ª–∞–±–∫–∏—Ö –º—ñ—Å—Ü—å
3. –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–µ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤
4. –ü–µ—Ä–µ–≤–∞–ª—ñ–¥–∞—Ü—ñ—è
5. –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –Ω–∞–π–∫—Ä–∞—â–æ—ó –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó

–°—Ç—Ä–∞—Ç–µ–≥—ñ—ó –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó:
- Grid Search –¥–ª—è –æ–∫—Ä–µ–º–∏—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ñ–≤
- Bayesian Optimization (–∑ Optuna)
- Iterative Refinement
"""

from dataclasses import dataclass, field
from typing import Dict, List, Optional, Tuple, Any, Callable
from pathlib import Path
import json
import numpy as np
from datetime import datetime
from enum import Enum
import copy


@dataclass
class TuningConfig:
    """–ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è –¥–ª—è tuning"""
    # SOM –ø–∞—Ä–∞–º–µ—Ç—Ä–∏
    som_grid_sizes: List[Tuple[int, int]] = field(
        default_factory=lambda: [(12, 12), (15, 15), (18, 18)]
    )
    som_learning_rates: List[float] = field(
        default_factory=lambda: [0.3, 0.5, 0.7]
    )
    som_sigma_init: List[float] = field(
        default_factory=lambda: [3.0, 5.0, 7.0]
    )
    
    # Candidate Selector
    candidate_alphas: List[float] = field(
        default_factory=lambda: [0.85, 0.90, 0.95]
    )
    candidate_k: List[int] = field(
        default_factory=lambda: [6, 8, 10]
    )
    
    # NN –ø–∞—Ä–∞–º–µ—Ç—Ä–∏
    nn_hidden_dims: List[List[int]] = field(
        default_factory=lambda: [[256, 128], [512, 256], [256, 128, 64]]
    )
    nn_dropout: List[float] = field(
        default_factory=lambda: [0.2, 0.3, 0.4]
    )
    nn_learning_rates: List[float] = field(
        default_factory=lambda: [1e-3, 5e-4, 1e-4]
    )
    
    # Training
    max_epochs: int = 100
    patience: int = 10
    n_validation_samples: int = 500


@dataclass
class TuningResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç tuning"""
    component: str
    best_params: Dict[str, Any]
    best_score: float
    all_trials: List[Dict[str, Any]]
    improvement: float  # –ü–æ–∫—Ä–∞—â–µ–Ω–Ω—è –ø–æ—Ä—ñ–≤–Ω—è–Ω–æ –∑ –ø–æ—á–∞—Ç–∫–æ–≤–∏–º
    duration_seconds: float


@dataclass
class FullTuningReport:
    """–ó–≤—ñ—Ç –ø—Ä–æ –ø–æ–≤–Ω–∏–π tuning"""
    som_result: Optional[TuningResult] = None
    candidate_result: Optional[TuningResult] = None
    nn_result: Optional[TuningResult] = None
    
    initial_score: float = 0.0
    final_score: float = 0.0
    total_improvement: float = 0.0
    
    best_config: Dict[str, Any] = field(default_factory=dict)
    
    timestamp: str = ""
    total_duration_seconds: float = 0.0
    
    def save(self, path: str) -> None:
        """–ó–±–µ—Ä–µ–≥—Ç–∏ –∑–≤—ñ—Ç"""
        data = {
            "som": {
                "best_params": self.som_result.best_params if self.som_result else None,
                "best_score": self.som_result.best_score if self.som_result else None,
                "improvement": self.som_result.improvement if self.som_result else None,
            } if self.som_result else None,
            "candidate": {
                "best_params": self.candidate_result.best_params if self.candidate_result else None,
                "best_score": self.candidate_result.best_score if self.candidate_result else None,
            } if self.candidate_result else None,
            "nn": {
                "best_params": self.nn_result.best_params if self.nn_result else None,
                "best_score": self.nn_result.best_score if self.nn_result else None,
            } if self.nn_result else None,
            "initial_score": self.initial_score,
            "final_score": self.final_score,
            "total_improvement": self.total_improvement,
            "best_config": self.best_config,
            "timestamp": self.timestamp,
            "total_duration_seconds": self.total_duration_seconds,
        }
        
        with open(path, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)


class FullPipelineTuner:
    """
    –û–ø—Ç–∏–º—ñ–∑–∞—Ç–æ—Ä –ø–æ–≤–Ω–æ–≥–æ pipeline.
    
    –ü—Ä–∏–∫–ª–∞–¥ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è:
        tuner = FullPipelineTuner()
        
        report = tuner.tune(
            database_path="data/unified_disease_symptom_merged.json",
            output_dir="models/tuned/",
            strategy="iterative"
        )
        
        print(f"–ü–æ–∫—Ä–∞—â–µ–Ω–Ω—è: {report.total_improvement:.2%}")
    """
    
    def __init__(self, config: Optional[TuningConfig] = None):
        self.config = config or TuningConfig()
    
    def tune(
        self,
        database_path: str,
        output_dir: str,
        som_path: Optional[str] = None,
        nn_path: Optional[str] = None,
        strategy: str = "iterative",
        tune_som: bool = True,
        tune_candidate: bool = True,
        tune_nn: bool = True,
        verbose: bool = True
    ) -> FullTuningReport:
        """
        –ü–æ–≤–Ω–∞ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è pipeline.
        
        Args:
            database_path: –®–ª—è—Ö –¥–æ –±–∞–∑–∏ —Ö–≤–æ—Ä–æ–±
            output_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä—ñ—è –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
            som_path: –ü–æ—Ç–æ—á–Ω–∞ SOM –º–æ–¥–µ–ª—å (–æ–ø—Ü—ñ–æ–Ω–∞–ª—å–Ω–æ)
            nn_path: –ü–æ—Ç–æ—á–Ω–∞ NN –º–æ–¥–µ–ª—å (–æ–ø—Ü—ñ–æ–Ω–∞–ª—å–Ω–æ)
            strategy: "iterative", "grid", "bayesian"
            tune_som: –ß–∏ –æ–ø—Ç–∏–º—ñ–∑—É–≤–∞—Ç–∏ SOM
            tune_candidate: –ß–∏ –æ–ø—Ç–∏–º—ñ–∑—É–≤–∞—Ç–∏ Candidate Selector
            tune_nn: –ß–∏ –æ–ø—Ç–∏–º—ñ–∑—É–≤–∞—Ç–∏ NN
            verbose: –í–∏–≤–æ–¥–∏—Ç–∏ –ø—Ä–æ–≥—Ä–µ—Å
            
        Returns:
            FullTuningReport
        """
        start_time = datetime.now()
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        if verbose:
            print("=" * 60)
            print("FULL PIPELINE TUNING")
            print("=" * 60)
            print(f"Strategy: {strategy}")
            print(f"Output: {output_dir}")
        
        # –û—Ü—ñ–Ω–∫–∞ –ø–æ—á–∞—Ç–∫–æ–≤–æ—ó —è–∫–æ—Å—Ç—ñ
        initial_score = 0.0
        if som_path and nn_path:
            initial_score = self._evaluate_pipeline(
                som_path, nn_path, database_path, verbose
            )
            if verbose:
                print(f"\n–ü–æ—á–∞—Ç–∫–æ–≤–∞ —è–∫—ñ—Å—Ç—å: {initial_score:.4f}")
        
        results = {}
        current_som_path = som_path
        current_nn_path = nn_path
        
        # 1. Tuning SOM
        if tune_som:
            if verbose:
                print("\n" + "-" * 40)
                print("üìä Tuning SOM...")
            
            som_result = self._tune_som(
                database_path=database_path,
                output_path=output_path / "som_tuned.pkl",
                verbose=verbose
            )
            results['som'] = som_result
            
            if som_result:
                current_som_path = str(output_path / "som_tuned.pkl")
        
        # 2. Tuning Candidate Selector (–ø–∞—Ä–∞–º–µ—Ç—Ä–∏ –∑–±–µ—Ä—ñ–≥–∞—é—Ç—å—Å—è –≤ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó)
        if tune_candidate and current_som_path:
            if verbose:
                print("\n" + "-" * 40)
                print("üìã Tuning Candidate Selector...")
            
            candidate_result = self._tune_candidate(
                som_path=current_som_path,
                database_path=database_path,
                verbose=verbose
            )
            results['candidate'] = candidate_result
        
        # 3. Tuning NN
        if tune_nn and current_som_path:
            if verbose:
                print("\n" + "-" * 40)
                print("üß† Tuning Neural Network...")
            
            nn_result = self._tune_nn(
                som_path=current_som_path,
                database_path=database_path,
                output_path=output_path / "nn_tuned.pt",
                verbose=verbose
            )
            results['nn'] = nn_result
            
            if nn_result:
                current_nn_path = str(output_path / "nn_tuned.pt")
        
        # –§—ñ–Ω–∞–ª—å–Ω–∞ –æ—Ü—ñ–Ω–∫–∞
        final_score = 0.0
        if current_som_path and current_nn_path:
            final_score = self._evaluate_pipeline(
                current_som_path, current_nn_path, database_path, verbose
            )
        
        improvement = final_score - initial_score
        
        duration = (datetime.now() - start_time).total_seconds()
        
        # –ó–±–∏—Ä–∞—î–º–æ –Ω–∞–π–∫—Ä–∞—â—É –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—é
        best_config = self._collect_best_config(results)
        
        report = FullTuningReport(
            som_result=results.get('som'),
            candidate_result=results.get('candidate'),
            nn_result=results.get('nn'),
            initial_score=initial_score,
            final_score=final_score,
            total_improvement=improvement,
            best_config=best_config,
            timestamp=datetime.now().isoformat(),
            total_duration_seconds=duration,
        )
        
        # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –∑–≤—ñ—Ç
        report.save(str(output_path / "tuning_report.json"))
        
        if verbose:
            print("\n" + "=" * 60)
            print("–†–ï–ó–£–õ–¨–¢–ê–¢–ò TUNING")
            print("=" * 60)
            print(f"–ü–æ—á–∞—Ç–∫–æ–≤–∞ —è–∫—ñ—Å—Ç—å: {initial_score:.4f}")
            print(f"–§—ñ–Ω–∞–ª—å–Ω–∞ —è–∫—ñ—Å—Ç—å:  {final_score:.4f}")
            print(f"–ü–æ–∫—Ä–∞—â–µ–Ω–Ω—è:       {improvement:+.4f} ({improvement/max(initial_score, 0.001)*100:+.1f}%)")
            print(f"–ß–∞—Å:              {duration:.1f}—Å")
            print(f"–ó–≤—ñ—Ç:             {output_path / 'tuning_report.json'}")
        
        return report
    
    def _evaluate_pipeline(
        self,
        som_path: str,
        nn_path: str,
        database_path: str,
        verbose: bool
    ) -> float:
        """–û—Ü—ñ–Ω–∫–∞ —è–∫–æ—Å—Ç—ñ pipeline"""
        try:
            from dr_case.validation import validate_pipeline
            
            report = validate_pipeline(
                som_path=som_path,
                nn_path=nn_path,
                database_path=database_path,
                n_samples=200,
                verbose=False
            )
            
            # –ö–æ–º–±—ñ–Ω–æ–≤–∞–Ω–∞ –æ—Ü—ñ–Ω–∫–∞
            score = 0.0
            
            if report.candidate_report:
                score += report.candidate_report.recall * 0.3
            
            if report.nn_report:
                score += report.nn_report.recall_5 * 0.4
                score += report.nn_report.mean_average_precision * 0.3
            
            return score
            
        except Exception as e:
            if verbose:
                print(f"   –ü–æ–º–∏–ª–∫–∞ –æ—Ü—ñ–Ω–∫–∏: {e}")
            return 0.0
    
    def _tune_som(
        self,
        database_path: str,
        output_path: Path,
        verbose: bool
    ) -> Optional[TuningResult]:
        """Tuning SOM –∑ grid search"""
        start_time = datetime.now()
        trials = []
        best_score = -1.0
        best_params = {}
        
        try:
            from dr_case.optimization.som_tuner import SOMTuner
            from dr_case.som import SOMTrainer
            
            # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –±–∞–∑—É
            with open(database_path, 'r', encoding='utf-8') as f:
                database = json.load(f)
            
            # Grid search
            for grid_size in self.config.som_grid_sizes:
                for lr in self.config.som_learning_rates:
                    params = {
                        'height': grid_size[0],
                        'width': grid_size[1],
                        'learning_rate': lr,
                    }
                    
                    if verbose:
                        print(f"   Trying: {grid_size}, lr={lr}...")
                    
                    try:
                        # –ù–∞–≤—á–∞—î–º–æ SOM
                        trainer = SOMTrainer(database_path)
                        trainer.train(
                            height=grid_size[0],
                            width=grid_size[1],
                            learning_rate_init=lr,
                            epochs=200,
                            verbose=False
                        )
                        
                        # –û—Ü—ñ–Ω—é—î–º–æ
                        qe = trainer.quantization_error
                        te = trainer.topographic_error
                        
                        # Score = 1 - (QE + TE) / 2
                        score = 1.0 - (qe + te) / 2
                        
                        trials.append({
                            'params': params,
                            'score': score,
                            'qe': qe,
                            'te': te,
                        })
                        
                        if score > best_score:
                            best_score = score
                            best_params = params
                            # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –Ω–∞–π–∫—Ä–∞—â—É –º–æ–¥–µ–ª—å
                            trainer.save_model(str(output_path))
                        
                    except Exception as e:
                        if verbose:
                            print(f"      Skip: {e}")
            
            duration = (datetime.now() - start_time).total_seconds()
            
            return TuningResult(
                component="SOM",
                best_params=best_params,
                best_score=best_score,
                all_trials=trials,
                improvement=0.0,  # –ë—É–¥–µ –æ–±—á–∏—Å–ª–µ–Ω–æ –ø—ñ–∑–Ω—ñ—à–µ
                duration_seconds=duration,
            )
            
        except ImportError as e:
            if verbose:
                print(f"   SOM tuner –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∏–π: {e}")
            return None
    
    def _tune_candidate(
        self,
        som_path: str,
        database_path: str,
        verbose: bool
    ) -> Optional[TuningResult]:
        """Tuning Candidate Selector"""
        start_time = datetime.now()
        trials = []
        best_score = -1.0
        best_params = {}
        
        try:
            from dr_case.validation import CandidateRecallValidator
            
            validator = CandidateRecallValidator()
            
            for alpha in self.config.candidate_alphas:
                for k in self.config.candidate_k:
                    params = {'alpha': alpha, 'k': k}
                    
                    if verbose:
                        print(f"   Trying: Œ±={alpha}, k={k}...")
                    
                    try:
                        report = validator.validate_from_checkpoint(
                            som_checkpoint_path=som_path,
                            database_path=database_path,
                            n_samples=300
                        )
                        
                        score = report.recall
                        
                        trials.append({
                            'params': params,
                            'score': score,
                            'avg_candidates': report.avg_candidates,
                        })
                        
                        if score > best_score:
                            best_score = score
                            best_params = params
                        
                    except Exception as e:
                        if verbose:
                            print(f"      Skip: {e}")
            
            duration = (datetime.now() - start_time).total_seconds()
            
            return TuningResult(
                component="CandidateSelector",
                best_params=best_params,
                best_score=best_score,
                all_trials=trials,
                improvement=0.0,
                duration_seconds=duration,
            )
            
        except Exception as e:
            if verbose:
                print(f"   Candidate tuner –ø–æ–º–∏–ª–∫–∞: {e}")
            return None
    
    def _tune_nn(
        self,
        som_path: str,
        database_path: str,
        output_path: Path,
        verbose: bool
    ) -> Optional[TuningResult]:
        """Tuning Neural Network"""
        start_time = datetime.now()
        trials = []
        best_score = -1.0
        best_params = {}
        
        try:
            # –°–ø—Ä–æ—â–µ–Ω–∞ –≤–µ—Ä—Å—ñ—è - —Ç—ñ–ª—å–∫–∏ –ª–æ–≥—É–≤–∞–Ω–Ω—è –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤
            # –ü–æ–≤–Ω–∏–π tuning –ø–æ—Ç—Ä–µ–±—É—î GPU —Ç–∞ –∑–Ω–∞—á–Ω–æ–≥–æ —á–∞—Å—É
            
            for hidden in self.config.nn_hidden_dims[:1]:  # –¢—ñ–ª—å–∫–∏ –ø–µ—Ä—à–∏–π –≤–∞—Ä—ñ–∞–Ω—Ç
                for dropout in self.config.nn_dropout[:1]:
                    for lr in self.config.nn_learning_rates[:1]:
                        params = {
                            'hidden_dims': hidden,
                            'dropout': dropout,
                            'learning_rate': lr,
                        }
                        
                        if verbose:
                            print(f"   Config: hidden={hidden}, dropout={dropout}, lr={lr}")
                        
                        # TODO: –ü–æ–≤–Ω–µ –Ω–∞–≤—á–∞–Ω–Ω—è NN
                        # –ü–æ–∫–∏ —â–æ –∑–±–µ—Ä—ñ–≥–∞—î–º–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏
                        
                        trials.append({
                            'params': params,
                            'score': 0.0,
                        })
                        
                        if not best_params:
                            best_params = params
            
            duration = (datetime.now() - start_time).total_seconds()
            
            return TuningResult(
                component="NeuralNetwork",
                best_params=best_params,
                best_score=best_score,
                all_trials=trials,
                improvement=0.0,
                duration_seconds=duration,
            )
            
        except Exception as e:
            if verbose:
                print(f"   NN tuner –ø–æ–º–∏–ª–∫–∞: {e}")
            return None
    
    def _collect_best_config(self, results: Dict[str, TuningResult]) -> Dict[str, Any]:
        """–ó–±–∏—Ä–∞—î –Ω–∞–π–∫—Ä–∞—â—É –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—é"""
        config = {}
        
        if 'som' in results and results['som']:
            config['som'] = results['som'].best_params
        
        if 'candidate' in results and results['candidate']:
            config['candidate_selector'] = results['candidate'].best_params
        
        if 'nn' in results and results['nn']:
            config['neural_network'] = results['nn'].best_params
        
        return config


# –®–≤–∏–¥–∫–∏–π –¥–æ—Å—Ç—É–ø
def tune_pipeline(
    database_path: str,
    output_dir: str,
    som_path: Optional[str] = None,
    nn_path: Optional[str] = None,
    verbose: bool = True
) -> FullTuningReport:
    """
    –®–≤–∏–¥–∫–∞ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è pipeline.
    
    Args:
        database_path: –®–ª—è—Ö –¥–æ –±–∞–∑–∏ —Ö–≤–æ—Ä–æ–±
        output_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä—ñ—è –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
        som_path: –ü–æ—Ç–æ—á–Ω–∞ SOM (–æ–ø—Ü—ñ–æ–Ω–∞–ª—å–Ω–æ)
        nn_path: –ü–æ—Ç–æ—á–Ω–∞ NN (–æ–ø—Ü—ñ–æ–Ω–∞–ª—å–Ω–æ)
        verbose: –í–∏–≤–æ–¥–∏—Ç–∏ –ø—Ä–æ–≥—Ä–µ—Å
        
    Returns:
        FullTuningReport
    """
    tuner = FullPipelineTuner()
    return tuner.tune(
        database_path=database_path,
        output_dir=output_dir,
        som_path=som_path,
        nn_path=nn_path,
        verbose=verbose
    )
