---
layout: default
title: "Частина IV: Формальні доведення гарантій"
nav_order: 3
---

# ЧАСТИНА IV. ФОРМАЛЬНІ ДОВЕДЕННЯ ГАРАНТІЙ CANDIDATE SELECTOR

[← Повернутися на головну](../)

---

## 14. Формальні означення для доведень

### 14.1. Простір симптомів

Нехай є фіксований словник симптомів розмірності $D$. Кожен випадок/діагноз кодується вектором у $\mathbb{R}^D$.

Вектор пацієнта (симптоми):
$$x_p \in \mathbb{R}^D$$

Вектор діагнозу $d$ (з бази знань):
$$x_d \in \mathbb{R}^D$$

### 14.2. SOM-структура

Є множина нейронів (юнітів) $U$ і прототипи:
$$w_u \in \mathbb{R}^D, \quad u \in U$$

BMU-функція:
$$\mathrm{BMU}(x) = \arg\min_{u \in U} \|x - w_u\|$$

### 14.3. Мапа "нейрон → діагнози"

На офлайн-кроці 5 ми будуємо відображення:
$$I: U \to 2^{\mathcal{D}}$$

таке, що для кожного діагнозу $d$:
$$d \in I(\mathrm{BMU}(x_d))$$

### 14.4. Candidate Selector (узагальнено)

Candidate Selector обирає множину юнітів $S(x_p) \subseteq U$ і повертає:
$$\mathcal{D}_{cand}(x_p) = \bigcup_{u \in S(x_p)} I(u)$$

---

## 15. Що означає "істинний діагноз не втрачено"

Нехай істинний діагноз пацієнта — $d^*$. "Не втрачено" означає:
$$d^* \in \mathcal{D}_{cand}(x_p)$$

---

## 16. Ключова лема (умова достатності)

**Лема.** Якщо існує юніт $u^* \in S(x_p)$ такий, що $d^* \in I(u^*)$, то $d^* \in \mathcal{D}_{cand}(x_p)$.

**Доведення.**

За означенням $\mathcal{D}_{cand}(x_p) = \bigcup_{u \in S(x_p)} I(u)$.

Якщо $u^* \in S(x_p)$, то $I(u^*)$ входить у це об'єднання.

Якщо $d^* \in I(u^*)$, то $d^*$ належить об'єднанню. ∎

Тобто вся задача зводиться до: **гарантувати, що Candidate Selector включив хоча б той юніт, до якого прив'язаний $d^*$**.

---

## 17. Теорема гарантії через радіус

Ця теорема дає 100% гарантію за "розумних параметрів", але потребує, щоб Candidate Selector вибирав юніти за порогом відстані (або еквівалентним йому порогом membership).

### 17.1. Припущення (реалістичні)

1. Пацієнтський вектор близький до вектора істинного діагнозу:
   $$\|x_p - x_{d^*}\| \leq \varepsilon_{\text{case}}$$
   (це формалізує "пацієнт описав симптоми без великої помилки".)

2. Для діагнозу $d^*$ SOM має обмежену квантизаційну помилку:
   $$\|x_{d^*} - w_{u^*}\| \leq \varepsilon_{\text{som}}$$
   де $u^* = \mathrm{BMU}(x_{d^*})$.
   (це означає: SOM добре апроксимує цей діагноз.)

3. Candidate Selector включає всі юніти в межах радіуса $r$:
   $$S(x_p) \supseteq \{u \in U:\ \|x_p - w_u\| \leq r\}$$

### 17.2. Твердження

Якщо
$$r \geq \varepsilon_{\text{case}} + \varepsilon_{\text{som}},$$
то
$$d^* \in \mathcal{D}_{cand}(x_p).$$

### 17.3. Доведення

Нехай $u^* = \mathrm{BMU}(x_{d^*})$. За побудовою мапи на кроці 5:
$$d^* \in I(u^*).$$

Оцінимо відстань від пацієнта до прототипу цього юніта:
$$\|x_p - w_{u^*}\| \leq \|x_p - x_{d^*}\| + \|x_{d^*} - w_{u^*}\| \leq \varepsilon_{\text{case}} + \varepsilon_{\text{som}} \leq r$$
(трикутна нерівність).

Отже $u^*$ належить множині юнітів в радіусі $r$, а значить $u^* \in S(x_p)$.
Тоді за Лемою $d^* \in \mathcal{D}_{cand}(x_p)$. ∎

**Інтерпретація:** якщо ти береш достатньо "широко" навколо пацієнта в SOM-просторі, істинний діагноз гарантовано потрапляє у кандидати.

---

## 18. Теорема для top-k політики

### 18.1. Формулювання

Нехай $u^* = \mathrm{BMU}(x_{d^*})$. Якщо $u^*$ входить у top-k юнітів для $x_p$, тоді істинний діагноз не втрачається:
$$u^* \in \mathrm{TopK}(x_p) \Rightarrow d^* \in \mathcal{D}_{cand}(x_p)$$

бо $d^* \in I(u^*)$ (крок 5) і $u^* \in S(x_p)$ (вибір).

### 18.2. Практично-формальна умова "розумного k"

Якщо існує розрив (separation) між відстанню до $u^*$ та відстанями до більшості інших юнітів, тобто кількість юнітів ближчих за $u^*$ не перевищує $k-1$, тоді $u^*$ буде в top-k. Формально:
$$\#\{u \in U:\ \|x_p - w_u\| < \|x_p - w_{u^*}\|\} \leq k - 1.$$

Це **і є** формалізація "k достатньо велике для даного випадку".

---

## 19. Формальний опис Candidate Selector як функції з параметрами $(k, \tau, \alpha)$

### 19.1. Вхідні об'єкти (формально)

**SOM-вихід (membership-only)**

Нехай для випадку пацієнта маємо список юнітів з membership:
$$R(x) = \big((u_1, m_1), (u_2, m_2), \dots, (u_n, m_n)\big)$$

де:
* $u_i \in U$ — юніт SOM
* $m_i \in (0, 1]$
* $m_1 \geq m_2 \geq \dots \geq m_n$
* $\sum_{i=1}^{n} m_i = 1$

**Фіксована мапа "юніт → діагнози"**
$$I: U \to 2^{\mathcal{D}}$$

### 19.2. Загальна форма Candidate Selector

Candidate Selector складається з двох кроків:

1. вибір множини юнітів $S \subseteq U$ за політикою $\pi(k, \tau, \alpha)$
2. повернення кандидатів як об'єднання діагнозів із цих юнітів:

$$\mathrm{Cand}(x; k, \tau, \alpha) = \bigcup_{u \in S(x; k, \tau, \alpha)} I(u)$$

---

## 20. Три політики вибору юнітів

### 20.1. Політика A: Top-$k$

$$S_k(x) = \{u_1, u_2, \dots, u_k\}$$

Це "беремо $k$ найкращих юнітів за membership".

### 20.2. Політика B: Threshold за $\tau$

$$S_\tau(x) = \{u_i:\ m_i \geq \tau\}$$

Це "беремо всі юніти, чия membership не менше $\tau$".

### 20.3. Політика C: Cumulative mass за $\alpha$ (найважливіша)

Нехай
$$K_\alpha(x) = \min\left\{K:\ \sum_{i=1}^{K} m_i \geq \alpha \right\}$$

Тоді
$$S_\alpha(x) = \{u_1, \dots, u_{K_\alpha(x)}\}$$

Це "беремо найменшу кількість top-юнітів, що разом покривають масу $\alpha$".

### 20.4. Комбінована політика (рекомендована)

Щоб використати всі три параметри $(k, \tau, \alpha)$ одночасно:

1. спочатку визначити $K_\alpha(x)$ (масовий критерій),
2. обрізати зверху $k$,
3. відкинути юніти нижче $\tau$.

Формально:
$$K'(x) = \min\big(k,\ K_\alpha(x)\big)$$
$$S(x; k, \tau, \alpha) = \{u_i:\ 1 \leq i \leq K'(x)\ \wedge\ m_i \geq \tau\}$$

А Candidate Selector:
$$\boxed{\mathrm{Cand}(x; k, \tau, \alpha) = \bigcup_{u \in S(x; k, \tau, \alpha)} I(u)}$$

---

## 21. Теорема про cumulative mass (строга форма)

### 21.1. Позначення і визначення

**Що таке SOM-юніт (unit)**

SOM — це карта розміру, наприклад, $10 \times 10$. Це означає, що є 100 нейронів карти. Кожен такий нейрон ми називаємо **юнітом SOM** (SOM unit).

Множина всіх юнітів:
$$U = \{u_1, u_2, \dots, u_M\}$$
де $M = 100$ для карти $10 \times 10$.

**Що таке membership**

Membership — це число:
$$m(u) \in (0, 1]$$
яке можна інтерпретувати як "частка уваги/схожості" до патерну $u$.

Ми працюємо з **впорядкованим списком** юнітів та їх membership:
$$R = \big((u_1, m_1), (u_2, m_2), \dots, (u_n, m_n)\big)$$

де:
1. $m_1 \geq m_2 \geq \dots \geq m_n$ (відсортовано за спаданням);
2. $\sum_{i=1}^{n} m_i = 1$ (нормалізовано).

**Що таке $S_\alpha$: політика cumulative mass**

Нехай $\alpha \in (0, 1)$ — заданий параметр.

Формально визначимо число:
$$K_\alpha = \min\left\{K \in \{1, \dots, n\}:\ \sum_{i=1}^{K} m_i \geq \alpha \right\}$$

Тобто $K_\alpha$ — **найменша** кількість перших юнітів, яка "покриває" масу $\alpha$.

Тоді множина обраних юнітів:
$$S_\alpha = \{u_1, u_2, \dots, u_{K_\alpha}\}$$

**Що таке "істинний юніт" $u^*$**

Нехай істинний діагноз пацієнта — $d^*$. На офлайн-кроці 5 ви прив'язали кожен діагноз з бази до одного юніта. Отже існує юніт $u^*$, такий що:
$$d^* \in I(u^*)$$

Саме $u^*$ ми називаємо **істинним юнітом** для цього діагнозу.

### 21.2. Формулювання теореми

**Теорема (достатня умова включення істинного юніта).**

Нехай $R = \big((u_1, m_1), \ldots, (u_n, m_n)\big)$ — membership-розподіл SOM, відсортований за спаданням, і $\sum_{i=1}^{n} m_i = 1$. Нехай $\alpha \in (0, 1)$, і $S_\alpha$ визначено через cumulative mass політику.

Нехай $u^*$ — деякий юніт з цього списку, і його membership позначимо $m(u^*)$.

**Якщо**
$$m(u^*) > 1 - \alpha$$
**то**
$$u^* \in S_\alpha$$

Це твердження **достатнє**: якщо умова виконується, включення гарантоване. Якщо умова не виконується, $u^*$ може все одно входити в $S_\alpha$, але теорема цього не гарантує.

### 21.3. Повне доведення (крок за кроком)

Почнемо з того, що ми хочемо довести включення $u^*$ у множину $S_\alpha$. Нагадаю:
$$S_\alpha = \{u_1, u_2, \ldots, u_{K_\alpha}\}$$
де $K_\alpha$ — найменше число, для якого сума перших $K_\alpha$ membership не менша за $\alpha$.

Отже, твердження $u^* \in S_\alpha$ еквівалентне твердженню:

> **юніт $u^*$ знаходиться серед перших $K_\alpha$ елементів відсортованого списку.**

Позначимо індекс юніта $u^*$ у впорядкованому списку як $t$. Тобто:
$$u^* = u_t, \quad m(u^*) = m_t$$

Ми хочемо довести:
$$t \leq K_\alpha$$

**Крок 1. Розглянемо суму membership без істинного юніта**

Оскільки $\sum_{i=1}^{n} m_i = 1$, то сума membership усіх юнітів, крім $u^*$, дорівнює:
$$\sum_{i \neq t} m_i = 1 - m_t$$

Це абсолютно точна рівність, без припущень.

**Крок 2. Використаємо умову теореми**

За умовою теореми:
$$m_t > 1 - \alpha$$

Перенесемо $m_t$ в інший бік нерівності:
$$1 - m_t < \alpha$$

Це означає:

> **Сумарна membership усіх юнітів, крім $u^*$, строго менша за $\alpha$.**

Тобто:
$$\sum_{i \neq t} m_i < \alpha$$

Зверніть увагу на **строгу** нерівність $<$ (не $\leq$).

**Крок 3. Припустимо супротивне (доведення від супротивного)**

Припустимо:
$$u^* \notin S_\alpha$$

Це означає, що навіть після того, як ми взяли перші $K_\alpha$ юнітів, $u^*$ серед них немає. Тобто індекс $t$ істинного юніта більший за $K_\alpha$:
$$t > K_\alpha$$

Інакше кажучи, ми набрали потрібну масу $\alpha$, використовуючи лише юніти з множини:
$$\{u_1, \ldots, u_{K_\alpha}\} \subseteq \{u_i : i \neq t\}$$

Тобто ми набрали масу $\alpha$ **не використовуючи $u^*$**.

**Крок 4. Порівняємо, яку масу можна набрати без $u^*$**

Якщо ми взагалі не використовуємо $u^*$, то максимально можлива сумарна membership, яку ми можемо набрати, — це:
$$\sum_{i \neq t} m_i = 1 - m_t$$

Ми вже показали у Кроці 2, що:
$$1 - m_t < \alpha$$

Отже, **без $u^*$ ми можемо набрати строго менше, ніж $\alpha$**.

**Крок 5. Оцінимо суму перших $K_\alpha$ юнітів**

Тепер згадаємо наше припущення з Кроку 3: $u^* \notin S_\alpha$, тобто $t > K_\alpha$.

Це означає, що серед перших $K_\alpha$ юнітів немає $u^*$:
$$\{u_1, \ldots, u_{K_\alpha}\} \subseteq \{u_i : i \neq t\}$$

Тому сума membership перших $K_\alpha$ юнітів є частиною суми всіх юнітів без $u^*$:
$$\sum_{i=1}^{K_\alpha} m_i \leq \sum_{i \neq t} m_i$$

Але ми знаємо з Кроку 2, що $\sum_{i \neq t} m_i < \alpha$. Отже:
$$\sum_{i=1}^{K_\alpha} m_i \leq \sum_{i \neq t} m_i < \alpha$$

Тобто:
$$\sum_{i=1}^{K_\alpha} m_i < \alpha$$

**Крок 6. Застосуємо означення $K_\alpha$**

За означенням $K_\alpha$, це **найменше** число $K$, для якого:
$$\sum_{i=1}^{K} m_i \geq \alpha$$

Зокрема, для самого $K_\alpha$ повинно виконуватись:
$$\sum_{i=1}^{K_\alpha} m_i \geq \alpha$$

**Крок 7. Отримуємо суперечність**

Порівняємо результати Кроку 5 і Кроку 6:

- З Кроку 5: $\sum_{i=1}^{K_\alpha} m_i < \alpha$
- З Кроку 6: $\sum_{i=1}^{K_\alpha} m_i \geq \alpha$

Це **пряма суперечність**: одне й те саме число не може бути одночасно строго меншим за $\alpha$ і не меншим за $\alpha$.
$$\sum_{i=1}^{K_\alpha} m_i < \alpha \quad \land \quad \sum_{i=1}^{K_\alpha} m_i \geq \alpha \quad \Rightarrow \quad \bot$$

**Крок 8. Висновок**

Оскільки припущення $u^* \notin S_\alpha$ призвело до суперечності, воно є хибним.

Отже, істинним є протилежне твердження:
$$u^* \in S_\alpha$$

$\blacksquare$

### 21.4. Чому строга нерівність важлива

| Умова | Результат Кроку 2 | Суперечність у Кроці 7 |
|-------|-------------------|------------------------|
| $m(u^*) \geq 1 - \alpha$ | $\sum_{i \neq t} m_i \leq \alpha$ | $< \alpha$ vs $\geq \alpha$ або $= \alpha$ vs $\geq \alpha$ |
| $m(u^*) > 1 - \alpha$ | $\sum_{i \neq t} m_i < \alpha$ | $< \alpha$ vs $\geq \alpha$ |

При **нестрогій** нерівності ($\geq$) у Кроці 2 отримуємо $\sum_{i \neq t} m_i \leq \alpha$. Тоді в Кроці 7 можлива ситуація:
$$\sum_{i=1}^{K_\alpha} m_i = \alpha$$

Це не є суперечністю з $\leq \alpha$, і тоді результат залежить від правил ties-breaking.

При **строгій** нерівності ($>$) у Кроці 2 отримуємо $\sum_{i \neq t} m_i < \alpha$. Тоді в Кроці 7 маємо **безумовну суперечність**.

---

## 22. Числові приклади для демонстрації теореми

### 22.1. Приклад 1: "гострий" розподіл (умова НЕ виконується)

Нехай $\alpha = 0.9$ і маємо такий розподіл:

```
u₁: 0.85
u₂: 0.08
u₃: 0.04   ← істинний u*
u₄: 0.03
```

Перевіримо умову теореми:
- $1 - \alpha = 1 - 0.9 = 0.1$
- $m(u^*) = 0.04$
- $0.04 > 0.1$? **Ні!**

Умова теореми **не виконується**, тому теорема не гарантує включення $u^*$.

Справді, $S_{0.9}$:
- $u_1$: сума $= 0.85 < 0.9$
- $u_1 + u_2$: сума $= 0.93 \geq 0.9$ ✓

Отже, $S_{0.9} = \{u_1, u_2\}$, і $u^* = u_3$ **не включений**.

### 22.2. Приклад 2: умова виконується

Нехай $\alpha = 0.9$ і маємо такий розподіл:

```
u₁: 0.45
u₂: 0.35
u₃: 0.15   ← істинний u*
u₄: 0.05
```

Перевіримо умову теореми:
- $1 - \alpha = 0.1$
- $m(u^*) = 0.15$
- $0.15 > 0.1$? **Так!**

Умова теореми **виконується**, тому $u^* \in S_{0.9}$ гарантовано.

Перевіримо:
- $u_1$: сума $= 0.45 < 0.9$
- $u_1 + u_2$: сума $= 0.80 < 0.9$
- $u_1 + u_2 + u_3$: сума $= 0.95 \geq 0.9$ ✓

Отже, $S_{0.9} = \{u_1, u_2, u_3\}$, і $u^* = u_3$ **включений** ✓

### 22.3. Приклад 3: "плаский" розподіл

Нехай $\alpha = 0.9$ і маємо:

```
u₁: 0.26
u₂: 0.25
u₃: 0.25
u₄: 0.24   ← істинний u*
```

Перевіримо умову:
- $1 - \alpha = 0.1$
- $m(u^*) = 0.24$
- $0.24 > 0.1$? **Так!**

Умова виконується, тому $u^*$ гарантовано в $S_{0.9}$.

Перевіримо:
- $u_1 + u_2 + u_3 = 0.76 < 0.9$
- $u_1 + u_2 + u_3 + u_4 = 1.0 \geq 0.9$ ✓

$S_{0.9} = \{u_1, u_2, u_3, u_4\}$, і $u^*$ **включений** ✓

### 22.4. Приклад 4: "довгий хвіст"

```
u₁: 0.14
u₂: 0.13
u₃: 0.13
u₄: 0.12
u₅: 0.12
u₆: 0.12
u₇: 0.12
u₈: 0.12   ← істинний u*
```

Перевіримо умову ($\alpha = 0.9$):
- $1 - \alpha = 0.1$
- $m(u^*) = 0.12$
- $0.12 > 0.1$? **Так!**

$S_\alpha$ при $\alpha = 0.9$ накопичує:
* після 6 юнітів: $0.14+0.13+0.13+0.12+0.12+0.12 = 0.76$
* після 7: $0.88$
* після 8: $1.00$ **(≥ 0.9)**

$S_{0.9} = \{u_1, \ldots, u_8\}$, і істинний $u_8$ включено ✓

---

## 23. Теорема про граничний випадок з ties

### 23.1. Означення

Нехай $m^* = m(u^*) = 1 - \alpha$ (граничне значення).

Визначимо класи еквівалентності:
- $T_{>} := \{u \in U : m(u) > m^*\}$ — юніти з більшою membership
- $T_{=} := \{u \in U : m(u) = m^*\}$ — юніти з такою самою membership
- $T_{<} := \{u \in U : m(u) < m^*\}$ — юніти з меншою membership

### 23.2. Теорема (три випадки)

Нехай $m(u^*) = 1 - \alpha$ і $M_{>} := \sum_{u \in T_{>}} m(u)$.

**Випадок 1:** Якщо $M_{>} \geq \alpha$, то $u^* \notin S_\alpha$.

**Випадок 2:** Якщо $M_{>} < \alpha$ і $N_{req} \geq |T_{=}|$, де $N_{req} = \lceil(\alpha - M_{>})/m^*\rceil$, то $u^* \in S_\alpha$.

**Випадок 3:** Якщо $M_{>} < \alpha$ і $N_{req} < |T_{=}|$, то результат залежить від порядку ties-breaking.

### 23.3. Доведення Випадку 1

Юніти з $T_{>}$ займають перші $|T_{>}|$ позицій у відсортованому списку.

Оскільки $M_{>} \geq \alpha$, то існує $k \leq |T_{>}|$ таке, що:
$$\sum_{i=1}^{k} m_i \geq \alpha$$

Отже, $K_\alpha \leq |T_{>}|$, і:
$$S_\alpha = \{u_1, \ldots, u_{K_\alpha}\} \subseteq T_{>}$$

Оскільки $u^* \in T_{=}$ і $T_{=} \cap T_{>} = \emptyset$, маємо $u^* \notin S_\alpha$. ∎

### 23.4. Доведення Випадку 2

Оскільки $M_{>} < \alpha$, юнітів з $T_{>}$ недостатньо для покриття маси $\alpha$. Потрібно додати юніти з $T_{=}$.

За умовою $N_{req} \geq |T_{=}|$. Це означає, що для покриття маси $\alpha$ потрібно взяти **всі** юніти з $T_{=}$, включаючи $u^*$. ∎

---

## 24. Розширена $\alpha$-політика $S_\alpha^+$

### 24.1. Означення

Визначимо **розширену α-політику** $S_\alpha^{+}$:
$$S_\alpha^{+} := \{u_i : 1 \leq i \leq K_\alpha^{+}\}$$

де 
$$K_\alpha^{+} := \max\{j : m_j = m_{K_\alpha}\}$$

тобто ми включаємо **всі** юніти з membership, рівним membership останнього включеного юніта.

### 24.2. Теорема

Для розширеної α-політики $S_\alpha^{+}$:

Якщо $m(u^*) \geq 1 - \alpha$, то $u^* \in S_\alpha^{+}$ **при будь-якому** початковому впорядкуванні.

### 24.3. Доведення

**Крок 1.** За означенням $S_\alpha^{+}$ включає всі юніти з membership $\geq m_{K_\alpha}$.

**Крок 2.** Нехай $m^* = m(u^*)$. Розглянемо два випадки:

**Випадок A:** $m^* > m_{K_\alpha}$

Тоді $u^*$ знаходиться серед перших $K_\alpha$ юнітів за означенням сортування, отже $u^* \in S_\alpha \subseteq S_\alpha^{+}$.

**Випадок B:** $m^* \leq m_{K_\alpha}$

Покажемо, що $m^* = m_{K_\alpha}$.

Від супротивного: нехай $m^* < m_{K_\alpha}$.

Обчислимо суму membership юнітів з більшим membership ніж $u^*$:
$$\sum_{u : m(u) > m^*} m(u) \geq \sum_{i=1}^{K_\alpha} m_i \geq \alpha$$

Але за умовою теореми $m^* \geq 1 - \alpha$, отже:
$$\sum_{u \neq u^*} m(u) = 1 - m^* \leq \alpha$$

Суперечність.

**Крок 3.** Якщо $m^* \geq m_{K_\alpha}$, то або $u^* \in \{u_1, \ldots, u_{K_\alpha}\}$, або $m^* = m_{K_\alpha}$.

У другому випадку за означенням $S_\alpha^{+}$ включає всі юніти з $m(u) = m_{K_\alpha}$, отже $u^* \in S_\alpha^{+}$. ∎

---

## 25. Теорема про нижню межу Candidate Recall

### 25.1. Визначення Candidate Recall

Нехай є набір тестових випадків:
$$\mathcal{T} = \{(c_j, d_j^*)\}_{j=1}^{N}$$

де:
* $c_j$ — клінічний випадок (симптоми),
* $d_j^* \in \mathcal{D}$ — істинний діагноз.

**Candidate recall** визначаємо як:
$$\mathrm{Rec}_{CS} = \frac{1}{N}\sum_{j=1}^{N}\mathbf{1}\{d_j^* \in \mathcal{D}_{cand}(c_j)\}$$

### 25.2. Означення "поганого" кейса

Кейс $j$ назвемо **поганим** (для даного $\alpha$), якщо:
$$\max_{u \in A(d_j^*)} m(u \mid x_j) < 1 - \alpha$$

де $A(d)$ — множина юнітів, прив'язаних до діагнозу $d$.

Позначимо множину поганих кейсів:
$$\mathcal{B}_\alpha = \left\{j: \max_{u \in A(d_j^*)} m(u \mid x_j) < 1 - \alpha\right\}$$

### 25.3. Теорема 2 (нижня межа Candidate Recall)

Для будь-якого набору кейсів $\mathcal{T}$ маємо:
$$\mathrm{Rec}_{CS} \geq 1 - \frac{|\mathcal{B}_\alpha|}{N}$$

### 25.4. Доведення

Для кожного "непоганого" кейса $j \notin \mathcal{B}_\alpha$ існує $u^* \in A(d_j^*)$ такий, що $m(u^* \mid x_j) \geq 1 - \alpha$.

За Теоремою 21 тоді $d_j^* \in \mathcal{D}_{cand}(x_j)$, тобто індикатор $\mathbf{1}\{\cdot\} = 1$.

Отже помилки можуть трапитися лише на кейсах з $\mathcal{B}_\alpha$.

Тому частка правильних не менша за $1 - |\mathcal{B}_\alpha|/N$. ∎

### 25.5. Практичний сенс

Це дає дуже сильний інструмент для аудиту:

> щоб підняти recall, вам не потрібно "магії" — потрібно зменшувати кількість кейсів, де істинний юніт має надто малу membership.

---

## 26. Теорема для комбінації $(\alpha, k, \tau)$

### 26.1. Теорема 3 (достатня умова)

Нехай для кейса з істинним діагнозом $d^*$ існує $u^* \in A(d^*)$ таке, що:

1. $m(u^* \mid x) \geq 1 - \alpha$ (як раніше),
2. $m(u^* \mid x) \geq \tau$,
3. $\mathrm{rank}(u^* \mid x) \leq k$.

Тоді $d^* \in \mathcal{D}_{cand}(x)$.

### 26.2. Доведення

(1) гарантує, що $u^* \in S_\alpha(x)$ (теорема 21).

(3) гарантує, що обрізання $\min(K_\alpha, k)$ не викине $u^*$.

(2) гарантує, що фільтр $m \geq \tau$ не викине $u^*$.

Отже $u^* \in S(x; k, \tau, \alpha)$.

Далі як у теоремі 16: $d^* \in I(u^*) \subseteq \mathcal{D}_{cand}(x)$. ∎

### 26.3. Висновок

* **Чистий $S_\alpha$** дає найкращі формальні гарантії.
* Додавання $k$ і $\tau$ — це інженерні запобіжники проти "хвоста", але вони завжди зменшують гарантійність.

---

## 27. Ймовірнісні гарантії (PAC-стиль)

### 27.1. Налаштування

Нехай клінічні випадки $(c, d^*)$ є випадковими величинами з деякого розподілу $P$.

Визначимо подію "істина не втрачена":
$$Z = \mathbf{1}\{d^* \in \mathcal{D}_{cand}(x(c))\}$$

Тоді справжній (популяційний) recall:
$$\mathrm{Rec}_{CS}^{(P)} = \mathbb{E}_P[Z]$$

На вибірці з $N$ незалежних кейсів емпіричний recall:
$$\widehat{\mathrm{Rec}}_{CS} = \frac{1}{N}\sum_{j=1}^N Z_j$$

### 27.2. Теорема 4 (концентрація Хеффдінга для recall)

Для будь-якого $\delta \in (0, 1)$ з імовірністю принаймні $1 - \delta$:
$$\mathrm{Rec}_{CS}^{(P)} \geq \widehat{\mathrm{Rec}}_{CS} - \sqrt{\frac{\ln(2/\delta)}{2N}}$$

### 27.3. Пояснення

Якщо на валідації recall = 0.995, то на "реальному світі" він буде **майже такий самий**, і різниця контролюється терміном $\sqrt{\ln(2/\delta)/(2N)}$. Чим більше $N$, тим менше "похибка гарантії".

---

## 28. Теорема через емпіричний квантиль рангу

### 28.1. Мотивація

Ранг істинного юніта $R(x) := \mathrm{rank}(u^*(x))$ визначає, чи буде істина в top-$k$:
$$R(x) \leq k \Leftrightarrow \text{істина не губиться через обрізання}$$

### 28.2. Теорема (гарантія через емпіричний квантиль)

Нехай $R$ — випадкова величина "ранг найкращого істинного SOM-юніта".

Нехай $R_1, \dots, R_N$ — незалежні спостереження на валідаційному наборі.

Позначимо:
* $F(r) := \Pr(R \leq r)$ — істинна функція розподілу рангу;
* $F_N(r) := \frac{1}{N}\sum_{j=1}^N \mathbf{1}\{R_j \leq r\}$ — емпірична функція розподілу.

Зафіксуємо $\varepsilon \in (0, 1)$ і $\delta \in (0, 1)$.

Визначимо $k$ як емпіричний $(1-\varepsilon)$-квантиль:
$$k := \min\{r:\ F_N(r) \geq 1 - \varepsilon\}$$

Тоді з імовірністю принаймні $1 - \delta$ виконується:
$$\Pr(R > k) \leq \varepsilon + \eta(N, \delta)$$

де
$$\eta(N, \delta) := \sqrt{\frac{\ln(2/\delta)}{2N}}$$

### 28.3. Повне доведення (через DKW-нерівність)

**Крок 1. Використаємо DKW-нерівність**

**Факт (DKW):** для будь-якого $\delta \in (0, 1)$ з імовірністю $\geq 1 - \delta$ виконується:
$$\sup_{r} |F_N(r) - F(r)| \leq \eta(N, \delta)$$

Позначимо подію:
$$E := \left\{\sup_r |F_N(r) - F(r)| \leq \eta(N, \delta)\right\}$$

Тоді:
$$\Pr(E) \geq 1 - \delta$$

**Крок 2. Використаємо означення $k$ як емпіричного квантиля**

За означенням:
$$k := \min\{r:\ F_N(r) \geq 1 - \varepsilon\}$$

Отже:
$$F_N(k) \geq 1 - \varepsilon$$

**Крок 3. Перенесемо на істинний розподіл**

На події $E$:
$$|F_N(k) - F(k)| \leq \eta$$

Звідси:
$$F(k) \geq F_N(k) - \eta \geq (1 - \varepsilon) - \eta$$

**Крок 4. Перейдемо до ймовірності "втрати"**

$$\Pr(R \leq k) = F(k) \geq 1 - \varepsilon - \eta$$

Тоді:
$$\Pr(R > k) = 1 - \Pr(R \leq k) \leq \varepsilon + \eta$$

**Крок 5. Врахуємо ймовірність події $E$**

Оскільки $\Pr(E) \geq 1 - \delta$, то з імовірністю $\geq 1 - \delta$:
$$\Pr(R > k) \leq \varepsilon + \eta(N, \delta)$$

$\blacksquare$

### 28.4. Практичний рецепт вибору $k$

Ціль: ймовірність втрати істини $\leq 0.1\%$, тобто $\Pr(R > k) \leq 0.001$.

За теоремою достатньо:
$$\varepsilon + \eta(N, \delta) \leq 0.001$$

Наприклад:
* вибрати $\varepsilon = 0.0005$,
* добитися, щоб $\eta(N, \delta) \leq 0.0005$.

---

## 29. Рекомендовані дефолтні параметри Candidate Selector

### 29.1. Параметр $\alpha$: рівень обережності

$\alpha$ відповідає на питання:

> *"Яку частину всієї невизначеності ми хочемо гарантовано покрити?"*

**Рекомендація: $\alpha = 0.9$**

При $\alpha = 0.9$:
* у "гострих" випадках береться 1–3 юніти;
* у "пласких" — автоматично береться більше;
* хвіст дрібних юнітів зазвичай не втягується.

### 29.2. Параметр $k$: верхня межа складності

$k$ — це **запобіжник**:

> *"Навіть якщо невизначеність висока, ми не готові аналізувати більше ніж $k$ патернів".*

**Рекомендація: $k = 6$ (production) або $k = 8$ (прототип)**

### 29.3. Параметр $\tau$: відсікання шуму

$\tau$ — поріг:

> *"Якщо вклад юніта надто малий, ми вважаємо його шумом".*

**Рекомендація: $\tau = 0.01$**

### 29.4. Базова політика

$$S(x) = \{u_i:\ 1 \leq i \leq \min(k, K_\alpha)\ \wedge\ m_i \geq \tau\}$$

з параметрами:
* $\alpha = 0.9$
* $k = 6$
* $\tau = 0.01$

---

## 30. Формалізація алгоритму вибору найкращого питання

### 30.1. Expected Information Gain

Для питання $s$ (симптому) обчислюємо:
$$\text{IG}(s) = H(m) - \mathbb{E}_{a}[H(m \mid s = a)]$$

де:
* $H(m) = -\sum_u m(u) \log m(u)$ — ентропія поточного membership
* $a \in \{0, 1\}$ — можлива відповідь
* $m \mid s = a$ — оновлений membership після відповіді

### 30.2. Формула оновлення Баєса

$$m(u \mid s = a) = \frac{m(u) \cdot P(s = a \mid u)}{\sum_v m(v) \cdot P(s = a \mid v)}$$

### 30.3. Правило вибору питання

$$s^* = \arg\max_{s \in Q} \text{IG}(s)$$

де $Q = \{s_i : x_i = 0\}$ — множина ще не заданих питань.

---

## 31. Повний алгоритм системи (псевдокод)

```
Input: patient symptoms x
Output: ranked diagnoses with confidence

1:  compute m(u | x) for all u in SOM
2:  S ← CandidateSelector(m, α, k, τ)
3:  D_cand ← ⋃_{u ∈ S} I(u)

4:  while uncertainty high do
5:      Q ← {s_i | x_i = 0}
6:      s* ← argmax IG(s_i | S, m)
7:      ask patient about s*
8:      update x
9:      recompute m(u | x)
10:     S ← CandidateSelector(m, α, k, τ)
11: end while

12: x_NN ← concat(x, m_S)
13: ŷ ← MultilabelNN(x_NN)

14: return ŷ restricted to D_cand
```

---

## 32. Підсумок формальних гарантій

1. Для **чистого $S_\alpha$** ви маєте детерміновану гарантію:
   якщо істинний юніт має membership $\geq 1 - \alpha$, істина **не губиться**.

2. Для датасету ви маєте нижню межу recall через частку "поганих" кейсів $|\mathcal{B}_\alpha|/N$.

3. Для $(\alpha, k, \tau)$ ви маєте достатню умову включення істини, але $k$ і $\tau$ додають нові причини втрати — і це треба контролювати емпірично.

4. Ви маєте ймовірнісну гарантію типу "валідаційний recall ≈ реальний recall" через концентрацію Хеффдінга.

5. Ви можете вибрати $k$ через емпіричний квантиль рангу з формальними гарантіями через DKW-нерівність.


---

